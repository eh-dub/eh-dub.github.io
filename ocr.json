{"metadata":{"title":"Digital Eyeballs; Using Optical Character Recognition on Video Games","date":"Aug 29, 2021","excerpt":"A practical guide to extracting information from video game streams."},"content":"<p>Given the proliferation of e-sports and Twitch streamers, I wonder if it would ever be useful to watch video games really fast. Like 12 hours in 12 seconds fast.</p>\n<h2>Let's Watch People Die</h2>\n<p>Call of Duty Warzone is one of the most popular games in the contemporary competitive scene. To win, you must kill. How might we watch every murder really fast?</p>\n<p><img src=\"posts/ocr/murder.png\" alt=\"Twitch Streamer HusKers kills an opponent\"></p>\n<p>Here we see Twitch Streamer HusKers finishing off an opponent. At the top-right of the screen is a kill counter. If we can read that number then we can extract every kill that HusKers makes. Here's a scaled up view of that kill counter just before HusKerrs makes that third kill.</p>\n<p><img src=\"posts/ocr/kill-count.png\" alt=\"Close-up of the kill counter\"></p>\n<p>Recognizing numbers is Machine Learning 101. Are we at a bar on a Friday night? 'cuz we about to get dem digits.</p>\n<h3>Step one: determine a bounding box for the region of interest</h3>\n<p>My preferred method is to overlay a grid and count the boxes. <a href=\"https://replit.com/@ehdub/Draw-a-grid-over-an-image?lite=true\">Here's a working example</a>.</p>\n<p><img src=\"posts/ocr/grid.png\" alt=\"Grid overlay of the murder\"></p>\n<p>These boxes are 20 pixels by 20 pixels. The source image is 1280x720 (<strong>i.e.</strong> 720p). Counting the squares we see that the digits are contained in the space (1212,30,1247,50).\nI did not get these numbers from the grid alone. The grid provides a starting point and after a few iterations you will discover a tight crop. The extra space is for double-digit numbers.</p>\n<h3>Step two: use bounding box to crop region of interest from video frames</h3>\n<p><a href=\"https://pypi.org/project/av/\">PyAV</a> is not the most inuitive library for video processing, but it is by far the fastest Python wrapper for FFmpeg that I've used. <a href=\"https://replit.com/@ehdub/Extract-ROIS-from-Video\">Here</a> is a live demo of how to use it to crop out the region of interest from the frames of a video.</p>\n<h3>Step three: configure a digital eyeball to perceive the digits</h3>\n<p><a href=\"https://tesseract-ocr.github.io\">Tesseract</a> is an open source \"optical character recognizer\". I think \"digital eyeball\" is a more intuitive name. I do not yet know how to build a REPL that runs Tesseract so <a href=\"https://github.com/eh-dub/tesseract-demo\">here is a GitHub repo</a> you can clone. Tesseract is invoked in the <code>run_ocr.sh</code> script. The command is presented on its own below:</p>\n<pre><code class=\"hljs language-apache\"><span class=\"hljs-attribute\">tesseract</span> imgs.txt results --dpi <span class=\"hljs-number\">70</span> --psm <span class=\"hljs-number\">7</span> -c tessedit_char_whitelist=<span class=\"hljs-number\">0123456789</span> tsv\n</code></pre>\n<ul>\n<li><code>imgs.txt</code> is a newline-separated-list of image filenames for tesseract to process.</li>\n<li><code>results</code> is the name of the results file</li>\n<li><code>--dpi 70</code> tells tesseract how many dots per inch the images are. DPI is relevant only for scanned images, but tesseract complains if you don't set this argument. 70 is tesseract's default when a DPI is not provided.</li>\n<li><code>--psm 7</code> refers to the \"page segmentation method\" <em>i.e.</em> how is the text laid out on the image. 7 is \"Treat the image as a single text line.\" <a href=\"https://tesseract-ocr.github.io/tessdoc/ImproveQuality.html#page-segmentation-method\">Read more here</a>.</li>\n<li><code>-c tessedit_char_whitelist=0123456789</code> defines the character set that can be recognized.</li>\n<li><code>tsv</code> report results as tab-seperated values</li>\n</ul>\n<p>The results reporting is the least user-friendly aspect of tesseract. At least in this example, images where characters are recognized have 5 result lines and images where characters are not recognized have 1 result line. Of the 5 result lines, just one contains the actual character recognized. I chose <code>tsv</code> for this demo because it at least includes no-recognition results. The default <code>txt</code> results reporting includes only recognitions so it is impossible to know which recognitions correspond to which inputs.</p>\n<h3>Putting it all together is the hard part</h3>\n<p>I have demonstrated transformations from video to cropped frames to recognized digits. In a future post I will demonstrate how to program against this event stream to pull out events of interest.</p>"}